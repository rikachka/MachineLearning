{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карацапова Ирина, 396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответе на вопросы своими словами (загугленный материал надо пересказать), ответ обоснуйте (напишите и ОБЪЯСНИТЕ формулки если потребуется), если не выходит, то вернитесь к лекции дополнительным материалам:\n",
    "\n",
    "**Вопрос 1**: Какие формулы у шума, смещения, разброса? Какой смысл у этих компонент?\n",
    "\n",
    "Шум - ошибка, связанная с несовершенством данных: погрешностью измерения, например. $\\mathbf{E}_{xy}[(y - \\mathbf{E}(y|x))^2]$ <br>\n",
    "Смещение - разность в математических ожиданиях реальных и полученных ответов. $\\mathbf{E}_{xy}[(\\mathbf{E}(y|x) - \\mathbf{E}_{X^l}(\\mu(X^l)))^2]$ <br>\n",
    "Разброс - изменение ответа, связанное с зависимостью от обучающей выборки. $\\mathbf{E}_{xy}\\mathbf{D}_{X^l}(\\mu(X^l))$\n",
    "\n",
    "**Вопрос 2**: 4. Приведите пример семейства с маленьким смещением и большим разбросом. Приведите пример семейства с большим смещением и маленьким разбросом.\n",
    "\n",
    "Маленькой смещение и большой разброс: DesicionTree большой глубины. <br>\n",
    "Решающие деревья большой глубины достаточно сложны для того, чтобы приближать непростые зависимости, поэтому смещение маленькое. Но при этом небольшое изменение в данных ведет к построению другого дерева (изменилось лучшее разбиение одной вершины на средней глубине => изменились все следующие вершины ниже по дереву), что говорит о большом разбросе.\n",
    "\n",
    "Большое смещение и маленький разброс: DesicionTree небольшой глубины.<br>\n",
    "Решающие деревья небольшой глубины еще недостаточно сложны для того, чтобы восстанавливать непростые зависимости (например, если данные зависят от сочетания 20 признаков, а у нас дерево глубины 1, то мы учтем только 1 признак). Но небольшое изменение данных не приведет к глобальному изменению дерева, так как лучшее разбиение не изменится. \n",
    "\n",
    "**Вопрос 3**: Как сгенерировать подвыборку с помощью бутстрапа?\n",
    "\n",
    "Пусть дана выборка длины L. Сгенерировать подвыборку с помощью бутстрапа значит L раз выбрать рандомный объект из исходной выборки. Полученная подвыборка так же окажется длины L и будет состоять из объектов исходной выборки с повторениями.\n",
    "\n",
    "**Вопрос 4**: Что такое бэггинг?\n",
    "\n",
    "Бэггинг - это подход к обучению алгоритмов композиции, где каждый алгоритм обучается независимо. Для каждого алгоритма из исходных данных формируется подвыборка такой же длины с повторениями, на которой он обучается. Далее выводы алгоритмов аггрегируются (происходит либо голосование за ответы, либо усреднение ответов). Подход построен таким образом с целью уменьшения корреляции между алгоритмами и, как следствие, уменьшеньшения разброса.\n",
    "\n",
    "**Вопрос 5**:  Как соотносятся смещение разброс композиции, построенной с помощью бэггинга, со смещением и разбросом одного базового алгоритма?\n",
    "\n",
    "Смещение не меняется у композиции по отношению к одному базовому алгоритму. Разброс уменьшается. \n",
    "\n",
    "**Вопрос 6**: Как обучается случайный лес? В чем отличия от обычной процедуры построения решающих деревьев?\n",
    "\n",
    "Обучение случайного леса - это независимое обучение набора деревьев комитета. Как происходит обучение отдельного дерева:\n",
    "\n",
    "1) Формируется бутстрап-подвыборка из исходной выборки, на которой будет происходить обучение.\n",
    "\n",
    "2) В каждом узле выбираем подмножество признаков, которые будем принимать во внимание (если исходное количество признаков d, то для задач классификации рекомендуется использовать $\\sqrt d$ признаков, а для задачи регрессии - $\\frac {d}{3}$ признаков). Находим наилучшее разбиение выборки по одному из указанных признаков, соответственно производим его и передаем управление в дочерние узлы для повторения процедуры.\n",
    "\n",
    "Таким образом, отличия заключаются в том, что каждое дерево строится на своей бутстрап-подвыборке, а внутри каждого узла выбираем подмножество признаков (для каждого узла это подмножество свое, они не зависят друг от друга).\n",
    "\n",
    "**Вопрос 7**: Почему хорошими базовыми алгоритмами для бэггинга являются именно деревья?\n",
    "\n",
    "Деревья очень сильно зависят от выборки и от того, какие разбиения произошли в вершинах. Они склонны к переобучению. При этом небольшое изменение данных (выбор фич, самих данных) приводит к кардинальному изменению в построении дерева, то есть между собой они слабо коррелируют. Таким образом, при помощи бэггинга можно добиться уменьшения разброса.\n",
    "\n",
    "**Вопрос 8**: Как оценить качество случайного леса с помощью out-of-bag-процедуры?\n",
    "Для каждого объекта оставляются те алгоритмы, которые не обучались на нем и считается несмещенная оценка обобщающей способности полученной композиции.\n",
    "\n",
    "-----------\n",
    "PS: Если проверяющий не понял ответ на большинство вопросов, то будет пичалька. Пишите так, чтобы можно было разобраться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Bagging</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Известно, что бэггинг плохо работает, если в качестве базовых классификаторов взять knn. Попробуем понять причины на простом примере.\n",
    "\n",
    "Пусть дана выборка $X^l$ из $l$ объектов с ответами из множества $Y = \\{−1, +1\\}$. Будем рассматривать классификатор одного ближайшего соседа в качестве базового алгоритма. Построим с помощью бэггинга композицию длины $N$:\n",
    "\n",
    "$$a_N(x) = sign(\\sum_{n=1}^{N} b_n(x))$$\n",
    "\n",
    "Оцените вероятность того, что ответ композиции на произвольном объекте x будет\n",
    "отличаться от ответа одного классификатора ближайшего соседа, обученного по всей\n",
    "выборке. Покажите, что эта вероятность стремится к нулю при N → ∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**<Решение>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Bagging Implementation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте беггинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class BaggingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_estimators, items_rate=1.0, features_rate=1.0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: sklearn.Classifier\n",
    "            Базовый алгоритм, который можно обучить (есть метод fit).\n",
    "            Для обучение композиции нужно много таких, можэно получить с помощю copy.deepcopy\n",
    "\n",
    "        n_estimators: int\n",
    "            Число алгоритмов в композиции\n",
    "\n",
    "        items_rate: float > 0\n",
    "            Доля объектов из трейна, на которой будет обучаться каждый базовый алгоритм\n",
    "\n",
    "        features_rate: float > 0\n",
    "            Доля фичей, на которой будет обучаться и применяться каждый базовый алгоритм\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.items_rate = items_rate\n",
    "        self.features_rate = features_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Метод должен обучить композицию алгоритмов, используя X, y как обучающую выборку.\n",
    "        Не забудте реализорвать функционал выбора случайных объектов и фичей.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "\n",
    "        # Тут храните обеченные базовые алгоритмы\n",
    "        self.estimators = []\n",
    "\n",
    "        # Тут храните фичи для каждого алгоритма\n",
    "        self.features_idx = []\n",
    "\n",
    "        self.feature_importances = [[] for i in range(X.shape[1])]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            # =======================================\n",
    "            # Обучите базовые алгоритмы\n",
    "            # =======================================\n",
    "            features_idx = random.sample(range(0, X.shape[1]), int(self.features_rate * X.shape[1]))\n",
    "            train_idx = [random.randint(0, len(y) - 1) for i in range(int(self.items_rate * len(y)))]\n",
    "            train = X[:, features_idx][train_idx]\n",
    "            self.estimators += [estimator.fit(train, y[train_idx])] \n",
    "            self.features_idx += [features_idx]\n",
    "            \n",
    "            score = accuracy_score(estimator.predict(train), y[train_idx])\n",
    "            for i in range(len(features_idx)):\n",
    "                train_new = train\n",
    "                np.random.shuffle(train_new[:, i])\n",
    "                score_new = accuracy_score(estimator.predict(train_new), y[train_idx])\n",
    "                feature = features_idx[i]\n",
    "                self.feature_importances[feature] += [abs(score - score_new)]\n",
    "            self.feature_importances = [np.mean(accs) for accs in self.feature_importances]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array матрица объекты признаки на которых нужно сказать ответ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: 1d np.array, Вектор классов для каждого объекта\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = [] # Храните тут ответы каждого базового алгоритма \n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # =======================================\n",
    "            # Получите ответы (вероятности) от всех базовых алгоритмов\n",
    "            # ======================================\n",
    "            test = X[:, self.features_idx[i]]  \n",
    "            probs.append(self.estimators[i].predict_proba(test))\n",
    "\n",
    "        # =======================================\n",
    "        # Усредните вероятности полученные от базовых алгоритмов\n",
    "        # =======================================\n",
    "        y_probs = reduce(lambda a,b: a + b, probs) / len(probs)\n",
    "        y_pred = [x.argmax() for x in y_probs]\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic = pd.read_csv('./data/train.csv')[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']]\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "titanic.Sex = sex_encoder.fit_transform(titanic.Sex)\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно обучить свой беггинг на датасете титаник, и посмотреть работает ли он. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948 0.772378516624\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 10 моделями\n",
    "# =======================================\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(), 10, 0.8, 0.8)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты:\n",
    "    - Работает-ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.98799999999999999, 0.79283887468030689)\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 100 моделями\n",
    "# =======================================\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(), 100)\n",
    "clf.fit(X_train, y_train)\n",
    "acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.80000000000000004, 0.76982097186700771)\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите LogsiticRegression \n",
    "# =======================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0.7544, 0.7749, 0.7928, 0.7928, 0.7826, 0.803, 0.7953, 0.7953]\n",
      "[0, 0, 0.7877, 0.7698, 0.8107, 0.8056, 0.8184, 0.8005, 0.8132, 0.7928]\n",
      "[0, 0, 0.7979, 0.78, 0.8081, 0.8107, 0.8132, 0.803, 0.7953, 0.8132]\n",
      "[0, 0, 0.7723, 0.7647, 0.8081, 0.7928, 0.8158, 0.8056, 0.8081, 0.8209]\n",
      "[0, 0, 0.7672, 0.7953, 0.8312, 0.803, 0.8056, 0.8056, 0.8081, 0.8184]\n",
      "[0, 0, 0.7928, 0.7877, 0.7928, 0.8107, 0.8056, 0.8209, 0.8056, 0.8005]\n",
      "[0, 0, 0.7902, 0.7928, 0.8005, 0.8107, 0.803, 0.8107, 0.7877, 0.8056]\n",
      "[0, 0, 0.7902, 0.7953, 0.8158, 0.803, 0.7826, 0.8005, 0.8107, 0.7953]\n",
      "[0, 0, 0.7826, 0.7749, 0.8056, 0.803, 0.8005, 0.7953, 0.803, 0.7851]\n",
      "[0, 0, 0.78, 0.7723, 0.803, 0.8056, 0.803, 0.8081, 0.8107, 0.7877]\n",
      "0.5 0.5 0.8312\n"
     ]
    }
   ],
   "source": [
    "items_rates = np.arange(0.1, 1.1, 0.1)\n",
    "features_rates = np.arange(0.1, 1.1, 0.1)\n",
    "accs = []\n",
    "for items_rate in items_rates:\n",
    "    accs_line = []\n",
    "    for features_rate in features_rates:\n",
    "        try:\n",
    "            clf = BaggingClassifier(DecisionTreeClassifier(), 100, items_rate, features_rate)\n",
    "            clf.fit(X_train, y_train)\n",
    "            acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "            accs_line.append(int(acc[1]*10000)/10000)\n",
    "        except ValueError:\n",
    "            accs_line.append(0)\n",
    "    accs.append(accs_line)\n",
    "    \n",
    "for accs_line in accs:\n",
    "    print(accs_line)\n",
    "# acc_best = max([max(accs_line) for accs_line in accs])\n",
    "# print(acc_best)\n",
    "\n",
    "accs_items_rate = [max(accs_line) for accs_line in accs]\n",
    "items_rate_index = accs_items_rate.index(max(accs_items_rate))\n",
    "items_rate_best = items_rates[items_rate_index]\n",
    "\n",
    "accs_features_rate = accs[items_rate_index]\n",
    "features_rate_index = accs_features_rate.index(max(accs_features_rate))\n",
    "features_rate_best = features_rates[features_rate_index]\n",
    "\n",
    "acc_best = accs[items_rate_index][features_rate_index]\n",
    "\n",
    "print(items_rate_best, features_rate_best, acc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, беггинг работает лучше, чем просто линейная модель.\n",
    "Найденные параметры: items_rate = 0.5, features_rate = 1.0.\n",
    "На самом деле, результат меняется от теста к тесту, глобально хороший items_rate колеблется от 0.3 до 0.5, a features_rate - от 0.5 до 1.0. Это вызвано тем, что выборки должны быть достаточно большими, чтобы выбрать из теста всю информацию, но и достаточно маленькими, чтобы уменьшить корреляцию. При этом нужно брать хотя бы половину признаков, а не по одному."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\n",
    "    './data/adult.data', \n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"], \n",
    "    header=None, na_values=\"?\")\n",
    "\n",
    "adult = pd.get_dummies(adult)\n",
    "adult[\"Target\"] = adult[\"Target_ >50K\"]\n",
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "X_train, y_train, X_test, y_test = X[:20000], y[:20000], X[20000:], y[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответте на вопросы:\n",
    "    - Работает-ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 0.85478863147838546)\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 100 моделями\n",
    "# =======================================\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(), 100)\n",
    "clf.fit(X_train, y_train)\n",
    "acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.79990000000000006, 0.79515962104927951)\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите LogsiticRegression \n",
    "# =======================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7698, 0.8404, 0.8369, 0.8465, 0.85]\n",
      "[0.7713, 0.8422, 0.8519, 0.8544, 0.8535]\n",
      "[0.757, 0.832, 0.8401, 0.8497, 0.8519]\n",
      "[0.7817, 0.8261, 0.8426, 0.8457, 0.8476]\n",
      "[0.7608, 0.8474, 0.8453, 0.854, 0.8464]\n",
      "0.3 0.7 0.8544\n"
     ]
    }
   ],
   "source": [
    "items_rates = np.arange(0.1, 1.1, 0.2)\n",
    "features_rates = np.arange(0.1, 1.1, 0.2)\n",
    "accs = []\n",
    "for items_rate in items_rates:\n",
    "    accs_line = []\n",
    "    for features_rate in features_rates:\n",
    "        try:\n",
    "            clf = BaggingClassifier(DecisionTreeClassifier(), 10, items_rate, features_rate)\n",
    "            clf.fit(X_train, y_train)\n",
    "            acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "            accs_line.append(int(acc[1]*10000)/10000)\n",
    "        except ValueError:\n",
    "            accs_line.append(0)\n",
    "    accs.append(accs_line)\n",
    "    \n",
    "for accs_line in accs:\n",
    "    print(accs_line)\n",
    "# acc_best = max([max(accs_line) for accs_line in accs])\n",
    "# print(acc_best)\n",
    "\n",
    "accs_items_rate = [max(accs_line) for accs_line in accs]\n",
    "items_rate_index = accs_items_rate.index(max(accs_items_rate))\n",
    "items_rate_best = items_rates[items_rate_index]\n",
    "\n",
    "accs_features_rate = accs[items_rate_index]\n",
    "features_rate_index = accs_features_rate.index(max(accs_features_rate))\n",
    "features_rate_best = features_rates[features_rate_index]\n",
    "\n",
    "acc_best = accs[items_rate_index][features_rate_index]\n",
    "\n",
    "print(items_rate_best, features_rate_best, acc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, беггинг снова работает лучше, чем просто линейная модель.\n",
    "Найденные параметры: items_rate = 0.3, features_rate = 0.7.\n",
    "Точно так же, следует выбирать чуть больше половины признаков и отбирать около 30% выборки, чтобы уменьшить вероятность корреляцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Text, Image Classification</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше в каждом эксперименте нужно: \n",
    "- сравниться с линейной моделью ( какую лучше выбрать?=) )\n",
    "- сделать выбор в пользу одной из моделей\n",
    "- выбор обосновать, почему одна из моделей хуже а другая лучше\n",
    "- что такое хуже и лучше\n",
    "- попробуйте беггинг над деревьями и линейными моделями \n",
    "- почему работает или не работает, какие особенности данных на это влияют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train, y_train = vectorizer.fit_transform(newsgroups_train.data), newsgroups_train.target\n",
    "X_test,  y_test  = vectorizer.transform(newsgroups_test.data), newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969860350009 0.827934147637\n",
      "CPU times: user 8.26 s, sys: 72.9 ms, total: 8.33 s\n",
      "Wall time: 8.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# =======================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.887926462789 0.762214551248\n",
      "CPU times: user 2min 18s, sys: 2.49 s, total: 2min 21s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над Линейную модель \n",
    "# =======================================\n",
    "\n",
    "clf = BaggingClassifier(LogisticRegression(), 100, 0.3, 0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.516174650875 0.379845990441\n",
      "CPU times: user 9.14 s, sys: 268 ms, total: 9.41 s\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=20)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731041187909 0.605682421668\n",
      "CPU times: user 2min 21s, sys: 1.95 s, total: 2min 23s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(max_depth=20), 100, 0.3, 0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная модель работает лучше решающих деревьев. При этом нет необходимости в беггинге. А у решающих деревьев - наоборот, беггинг помогает улучшить результаты, так как деревья не слишком коррелируют друг с другом. Линейная модель показала более качественный результат, так как она учитывает все признаки и меньше страдает от переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Image classification\n",
    "<br>Очень долго считалось... Я не дождалась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import load_cifar10\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10('./data/cifar10')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(X_train.shape[0], -1), X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# =======================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над Линейную модель \n",
    "# =======================================\n",
    "\n",
    "clf = BaggingClassifier(LogisticRegression(), 100, 0.3, 0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=20)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(max_depth=20), 100, 0.3, 0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Random Forest Feature Impotance</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Опишите как вычисляется важность фичей в дереве, можите изучить как работает  feature\\_importances_ в sklearn.\n",
    "\n",
    "---\n",
    "\n",
    "Перемешиваем значения для каждого признака. Чем сильнее изменилось accuracy, тем более он важен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Почитайте Feature Impotance для Adult и Titanic (используйте полный датасет), ПРОИНТЕРПРЕТИРУЙТЕ резульататы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "X_train, y_train, X_test, y_test = X[:20000], y[:20000], X[20000:], y[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                        items_rate=1, features_rate=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature  importance\n",
      "33        33    29.56290\n",
      "2          2    28.47405\n",
      "5          5    27.66570\n",
      "1          1    27.59955\n",
      "0          0    27.43540\n",
      "98        98    27.43330\n",
      "77        77    26.84605\n",
      "88        88    26.75770\n",
      "28        28    26.75080\n",
      "53        53    26.67895\n",
      "17        17    26.52255\n",
      "52        52    26.30850\n",
      "101      101    26.30055\n",
      "62        62    26.25730\n",
      "8          8    26.23570\n",
      "15        15    26.21745\n",
      "42        42    26.20505\n",
      "3          3    26.18725\n",
      "91        91    26.11220\n",
      "87        87    26.04660\n",
      "80        80    26.02665\n",
      "94        94    25.83300\n",
      "68        68    25.79215\n",
      "19        19    25.78435\n",
      "71        71    25.76035\n",
      "18        18    25.74305\n",
      "35        35    25.72395\n",
      "57        57    25.70380\n",
      "41        41    25.69275\n",
      "34        34    25.64535\n",
      "..       ...         ...\n",
      "82        82    24.17705\n",
      "95        95    24.16965\n",
      "66        66    24.16340\n",
      "105      105    24.10325\n",
      "16        16    23.94855\n",
      "103      103    23.79835\n",
      "104      104    23.79100\n",
      "26        26    23.78950\n",
      "20        20    23.76600\n",
      "55        55    23.72715\n",
      "70        70    23.72560\n",
      "85        85    23.66535\n",
      "75        75    23.62700\n",
      "102      102    23.58945\n",
      "69        69    23.52795\n",
      "84        84    23.50015\n",
      "92        92    23.46340\n",
      "97        97    23.41710\n",
      "25        25    23.34590\n",
      "107      107    23.33920\n",
      "38        38    23.33590\n",
      "96        96    23.23485\n",
      "6          6    23.09020\n",
      "106      106    23.07925\n",
      "13        13    23.04690\n",
      "23        23    22.97580\n",
      "79        79    22.79420\n",
      "22        22    22.72945\n",
      "37        37    22.28910\n",
      "89        89    21.69275\n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Посчитайте feature_importances для clf\n",
    "# =======================================\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = range(X_train.shape[1])\n",
    "feature_importances['importance'] = clf.feature_importances\n",
    "feature_importances = feature_importances.sort(['importance', 'feature'], ascending = False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Martial Status_ Married-spouse-absent', 'Capital Gain', 'Workclass_ ?',\n",
      "       'Education-Num', 'fnlwgt', 'Country_ Puerto-Rico', 'Country_ Greece',\n",
      "       'Country_ Jamaica', 'Education_ Prof-school',\n",
      "       'Relationship_ Not-in-family'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(adult.columns[feature_importances['feature'][:10] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Occupation_ Adm-clerical', 'Country_ Poland', 'Workclass_ Federal-gov',\n",
      "       'Country_ Yugoslavia', 'Workclass_ Without-pay', 'Education_ Bachelors',\n",
      "       'Country_ Haiti', 'Education_ Assoc-voc', 'Occupation_ ?',\n",
      "       'Country_ Japan'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(adult.columns[feature_importances['feature'][-10:] + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, все признаки важны в среднем равнозначно. Но основными являются отсутствие брака, капитал, образование. Все верно, все эти признаки свидетельствуют о бОльшем заработке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                        items_rate=1, features_rate=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature  importance\n",
      "1        1      38.904\n",
      "2        2      36.468\n",
      "0        0      35.354\n",
      "3        3      34.538\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Посчитайте feature_importances для clf\n",
    "# =======================================\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = range(X_train.shape[1])\n",
    "feature_importances['importance'] = clf.feature_importances\n",
    "feature_importances = feature_importances.sort(['importance', 'feature'], ascending = False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sex', 'Age', 'Pclass', 'Fare'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(titanic.columns[feature_importances['feature'] + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять же все признаки важны в среднем равнозначно. Пол и возраст оказались наиболее значимыми, так как первыми эвакуировали женщин и детей."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 Conda",
   "language": "python",
   "name": "py3_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
